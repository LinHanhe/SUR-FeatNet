{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from keras.applications import*\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils import multi_gpu_model\n",
    "import pickle\n",
    "from utils import *\n",
    "\n",
    "import os, sys\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract deep features for SUR prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call pre-trained inceptionv3 for deep feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hanhe/.conda/envs/hum/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "multigap_model = model_inception_multigap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from MCL-JCI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images\n",
      "1000 images\n",
      "2000 images\n",
      "3000 images\n",
      "4000 images\n",
      "5000 images\n",
      "6000 images\n",
      "7000 images\n",
      "8000 images\n",
      "9000 images\n",
      "10000 images\n",
      "11000 images\n",
      "12000 images\n",
      "13000 images\n",
      "14000 images\n",
      "15000 images\n",
      "16000 images\n",
      "17000 images\n",
      "18000 images\n",
      "19000 images\n",
      "20000 images\n",
      "21000 images\n",
      "22000 images\n",
      "23000 images\n",
      "24000 images\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('csv_file/jnd_10_fold.csv')\n",
    "feats_ref = []\n",
    "feats_dist = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    im_ref_path = 'MCL_JCI_patches/reference_patches/' + df.iloc[i,1] + '.bmp'\n",
    "    im_dist_path = 'MCL_JCI_patches/distorted_patches/' + df.iloc[i,2] + '.png'\n",
    "    \n",
    "    im_ref = Image.open(im_ref_path)\n",
    "    im_ref = np.array(im_ref)/255.\n",
    "    im_ref = np.expand_dims(im_ref, axis=0)\n",
    "    feat_ref = multigap_model.predict(im_ref)\n",
    "    feats_ref.append(feat_ref)\n",
    "    \n",
    "    im_dist = Image.open(im_dist_path)\n",
    "    im_dist = np.array(im_dist)/255.\n",
    "    im_dist = np.expand_dims(im_dist, axis=0)\n",
    "    feat_dist = multigap_model.predict(im_dist)\n",
    "    feats_dist.append(feat_dist)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print('%d images' % (i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 10048)\n",
      "(25000, 10048)\n"
     ]
    }
   ],
   "source": [
    "feats_ref = np.squeeze(np.array(feats_ref),axis=1)\n",
    "feats_dist = np.squeeze(np.array(feats_dist),axis=1)\n",
    "print(feats_ref.shape)\n",
    "print(feats_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "sur = df[['first_SUR']].values # SUR ground truth for first JND\n",
    "print(sur.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train a shallow NN for SUR prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set_id: 1\n",
      "valid_set_id: 2\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "WARNING:tensorflow:From /home/hanhe/.conda/envs/hum/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/hanhe/.conda/envs/hum/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "0.052965000846106045\n",
      "test_set_id: 2\n",
      "valid_set_id: 3\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "0.06429984945606419\n",
      "test_set_id: 3\n",
      "valid_set_id: 4\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "0.05448957860550981\n",
      "test_set_id: 4\n",
      "valid_set_id: 5\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "0.06931097728023894\n",
      "test_set_id: 5\n",
      "valid_set_id: 6\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "0.04191579706849076\n",
      "test_set_id: 6\n",
      "valid_set_id: 7\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "0.056633137937783656\n",
      "test_set_id: 7\n",
      "valid_set_id: 8\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "0.04573457757237586\n",
      "test_set_id: 8\n",
      "valid_set_id: 9\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "0.06538678301560726\n",
      "test_set_id: 9\n",
      "valid_set_id: 10\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "0.11159468606250378\n",
      "test_set_id: 10\n",
      "valid_set_id: 1\n",
      "(20000, 10048)\n",
      "(20000, 10048)\n",
      "(20000, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "(2500, 10048)\n",
      "(2500, 10048)\n",
      "(2500, 1)\n",
      "0.06365998652109328\n"
     ]
    }
   ],
   "source": [
    "'''split for training, validation, and test'''\n",
    "all_result = []\n",
    "\n",
    "df = pd.read_csv('csv_file/jnd_10_fold.csv')\n",
    "for i in range(10):\n",
    "    if i is 9:\n",
    "        test_set_id = i+1\n",
    "        valid_set_id = 1\n",
    "    else:\n",
    "        valid_set_id = i+2\n",
    "        test_set_id = i+1\n",
    "\n",
    "    print('test_set_id:',test_set_id)\n",
    "    print('valid_set_id:',valid_set_id)\n",
    "    \n",
    "    test_ids = df.loc[df.set==test_set_id]\n",
    "    test_ids = test_ids.reset_index(drop=True)\n",
    "    \n",
    "    X_train_ref = feats_ref[(df.set!=valid_set_id)&(df.set!=test_set_id)]\n",
    "    X_train_dist = feats_dist[(df.set!=valid_set_id)&(df.set!=test_set_id)]\n",
    "    y_train = sur[(df.set!=valid_set_id)&(df.set!=test_set_id)]\n",
    "    print(X_train_ref.shape)\n",
    "    print(X_train_dist.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "\n",
    "    X_valid_ref = feats_ref[df.set==valid_set_id]\n",
    "    X_valid_dist = feats_dist[df.set==valid_set_id]\n",
    "    y_valid = sur[df.set==valid_set_id]\n",
    "    print(X_valid_ref.shape)\n",
    "    print(X_valid_dist.shape)\n",
    "    print(y_valid.shape)\n",
    "\n",
    "\n",
    "    X_test_ref = feats_ref[df.set==test_set_id]\n",
    "    X_test_dist = feats_dist[df.set==test_set_id]\n",
    "    y_test = sur[df.set==test_set_id]\n",
    "    print(X_test_ref.shape)\n",
    "    print(X_test_dist.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    model = fc_model()\n",
    "    model.compile(loss=keras.losses.mean_absolute_error,\n",
    "                  optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, amsgrad=False))\n",
    "\n",
    "    # checkpoint\n",
    "    filepath ='save_model/best_train.hdf5'\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_loss', \n",
    "                                             verbose=0, \n",
    "                                             save_best_only=True, \n",
    "                                             mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    history = model.fit([X_train_ref,X_train_dist],y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=30,\n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=([X_valid_ref,X_valid_dist],y_valid))\n",
    "    model.load_weights(filepath)\n",
    "\n",
    "    y_pred = model.predict([X_test_ref,X_test_dist], batch_size=128)\n",
    "    #print(y_pred)\n",
    "    print(np.mean(np.absolute(y_pred-y_test)))\n",
    "    K.clear_session()\n",
    "    df_result = pd.concat([test_ids.iloc[:,0:3],pd.DataFrame(y_test,columns=['gt_SUR']),\n",
    "                           pd.DataFrame(y_pred,columns=['pred_SUR'])],axis=1)\n",
    "    all_result.append(df_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
